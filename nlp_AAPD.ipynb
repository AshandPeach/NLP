{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1FZIq9TYGwI82_KvY9GRLCiR95yuq-h20",
      "authorship_tag": "ABX9TyNLQX+2uLrMZK6sf7taXZIO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshandPeach/NLP/blob/main/nlp_AAPD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f7sBXDae13U",
        "outputId": "387b7756-8402-4542-9b18-0d1596b02558"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTjhVNqCduyM",
        "outputId": "829d2c09-47ae-495e-be26-5211f2e2f175"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.22.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel, BertConfig"
      ],
      "metadata": {
        "id": "C7rKmIyafYwX"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_to_dict_values(dict, f):\n",
        "    for key, value in dict.items():\n",
        "        dict[key] = f(value)"
      ],
      "metadata": {
        "id": "qbrUd5sggR8y"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AAPDDataset(Dataset):\n",
        "    \"\"\"AAPD dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        self.data = pd.read_csv(self.path, sep='\\t', header=None)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(BERT_TYPE)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    @staticmethod\n",
        "    def target_to_tensor(target):\n",
        "        return torch.tensor([float(label) for label in target])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.tokenizer(self.data.iloc[idx, 1], return_tensors=\"pt\", max_length=512, padding=\"max_length\", truncation=True)\n",
        "        apply_to_dict_values(data, lambda x: x.flatten())\n",
        "        return data, AAPDDataset.target_to_tensor(self.data.iloc[idx, 0])"
      ],
      "metadata": {
        "id": "2exk9FNpfbsD"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BERT_TYPE = 'bert-base-uncased'"
      ],
      "metadata": {
        "id": "cYtwZXRmgHWR"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "jtH5wvNSHezq"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# percentage=0.1"
      ],
      "metadata": {
        "id": "bRMb6zWQHukq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = AAPDDataset('/content/drive/MyDrive/nlp/AAPD/train.tsv')\n",
        "val_dataset = AAPDDataset('/content/drive/MyDrive/nlp/AAPD/validation.tsv')\n",
        "test_dataset = AAPDDataset('/content/drive/MyDrive/nlp/AAPD/test.tsv')"
      ],
      "metadata": {
        "id": "7W9UUZ4Pfn6d"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset[0]"
      ],
      "metadata": {
        "id": "aODwDrdFHGAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 256\n",
        "N_CLASSES = test_dataset[0][1].shape[0]\n",
        "N_CLASSES"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q1dYMJmgDh5",
        "outputId": "858ac556-91d5-49e9-9a12-e06d1cb87674"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqkE-7zhgg7f",
        "outputId": "5447599c-1903-49e0-c045-fad66643ede2"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "# val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "# test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "2sschT9Uglua"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for testing\n",
        "train_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "kp-zjjriRksE"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = BertConfig.from_pretrained(BERT_TYPE)\n",
        "config.return_dict = True\n",
        "bert = BertModel.from_pretrained(BERT_TYPE, config=config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osJ7wvMGgm-V",
        "outputId": "5616224b-534e-4a26-dd5d-a7539e9abeed"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiLabelBert(nn.Module):\n",
        "    def __init__(self, bert_model, n_classes):\n",
        "        super(MultiLabelBert, self).__init__()\n",
        "        self.bert_model = bert_model\n",
        "        for param in self.bert_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.n_bert_features = bert_model.pooler.dense.out_features\n",
        "        self.n_classes = n_classes\n",
        "        self.dense = nn.Linear(self.n_bert_features, self.n_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        bert_output = self.bert_model(**inputs)\n",
        "        return self.dense(bert_output.pooler_output)"
      ],
      "metadata": {
        "id": "x3dw9mOygouh"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiLabelBert(bert, N_CLASSES).to(device)"
      ],
      "metadata": {
        "id": "_KBdxjWUgqcf"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "LEARNING_RATE = 1e-4"
      ],
      "metadata": {
        "id": "8fjsv-f7grkA"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = []\n",
        "b = []\n",
        "batch, target = next(iter(train_dataloader))\n",
        "a.extend(target.to('cpu').tolist())\n",
        "apply_to_dict_values(batch, lambda x: x.to(device))\n",
        "b.extend((torch.sigmoid(model(batch)) > 0.5).type(torch.DoubleTensor).to('cpu').tolist())\n",
        "len(b), len(b[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnm9mRoCj-vI",
        "outputId": "d6e1c190-4824-4fbb-efbc-a3e0b6417b30"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 54)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import hamming_loss\n",
        "\n",
        "# print( \"Classification report: \\n\", (classification_report(a, b)))\n",
        "print( \"F1 micro averaging:\",(hamming_loss(a, b)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlF6H472kG0R",
        "outputId": "f03c5f38-9537-4d59-f26b-b72b03f4d6b8"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 micro averaging: 0.6145833333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "b5EUFOa4wbNd"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# writer = SummaryWriter('/content/drive/MyDrive/nlp/logs')"
      ],
      "metadata": {
        "id": "pBqOC9KcwwhO"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "from sklearn.metrics import f1_score, hamming_loss\n",
        "\n",
        "def train_model(\n",
        "        model,\n",
        "        train_dataloader,\n",
        "        val_dataloader,\n",
        "        optimizer,\n",
        "        criterion,\n",
        "        n_epochs):\n",
        "\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        model.train()\n",
        "        total_loss = 0.\n",
        "        all_targets = []\n",
        "        all_preds = []\n",
        "        for batch, targets in tqdm.tqdm(train_dataloader, f\"Train epoch#{epoch}\", leave=True):\n",
        "\n",
        "            apply_to_dict_values(batch, lambda x: x.to(device))\n",
        "            targets = targets.to(device)\n",
        "            logits = model(batch)\n",
        "            all_targets.extend(targets.to('cpu').tolist())\n",
        "            all_preds.extend((torch.sigmoid(logits) > 0.5).type(torch.DoubleTensor).to('cpu').tolist())\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(logits, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # writer.add_scalar(\"Batch train loss\", loss.item())\n",
        "        #     print(\"Batch train loss\", loss.item())\n",
        "\n",
        "        print(\"ave_Train_loss\", total_loss / len(train_dataloader))\n",
        "        print(\"Train F1 (micro)\",(f1_score(all_targets, all_preds, average='micro')))\n",
        "        print(\"Train Hamming loss\",(hamming_loss(all_targets, all_preds)))\n",
        "            # writer.close()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            total_loss = 0.\n",
        "            all_targets = []\n",
        "            all_preds = []\n",
        "            for batch, targets in tqdm.tqdm(val_dataloader, f\"Val epoch#{epoch}\", leave=False):\n",
        "                apply_to_dict_values(batch, lambda x: x.to(device))\n",
        "                targets = targets.to(device)\n",
        "                logits = model(batch)\n",
        "                all_targets.extend(targets.to('cpu').tolist())\n",
        "                all_preds.extend((torch.sigmoid(logits) > 0.5).type(torch.DoubleTensor).to('cpu').tolist())\n",
        "                loss = criterion(logits, targets)\n",
        "                total_loss += loss.item()\n",
        "            \n",
        "            print(\"Validation loss\", total_loss / len(val_dataloader))\n",
        "            print(\"Validation F1 (micro)\",(f1_score(all_targets, all_preds, average='micro')))\n",
        "            print(\"Validation Hamming loss\",(hamming_loss(all_targets, all_preds)))"
      ],
      "metadata": {
        "id": "DjnMV4JUkJjX"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiLabelBert(bert, N_CLASSES).to(device)\n",
        "optimizer = optim.Adam(params=[p for p in model.parameters() if p.requires_grad], lr=LEARNING_RATE)\n",
        "train_model(model, train_dataloader, val_dataloader, optimizer, torch.nn.BCEWithLogitsLoss(), 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK6TYaVzlegO",
        "outputId": "ce44e07b-decd-4266-9d8a-b8245678821a"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch#1: 100%|██████████| 4/4 [00:40<00:00, 10.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ave_Train_loss 0.6688603013753891\n",
            "Train F1 (micro) 0.08407587119541243\n",
            "Train Hamming loss 0.38451851851851854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss 0.6231765449047089\n",
            "Validation F1 (micro) 0.08529210050501901\n",
            "Validation Hamming loss 0.2716851851851852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch#2: 100%|██████████| 4/4 [00:41<00:00, 10.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ave_Train_loss 0.6083783507347107\n",
            "Train F1 (micro) 0.0726465109614558\n",
            "Train Hamming loss 0.2397037037037037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss 0.563126266002655\n",
            "Validation F1 (micro) 0.06115965051628276\n",
            "Validation Hamming loss 0.1751111111111111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch#3: 100%|██████████| 4/4 [00:41<00:00, 10.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ave_Train_loss 0.5525629073381424\n",
            "Train F1 (micro) 0.06389010030527693\n",
            "Train Hamming loss 0.159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss 0.509456068277359\n",
            "Validation F1 (micro) 0.06361323155216284\n",
            "Validation Hamming loss 0.12266666666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch#4: 100%|██████████| 4/4 [00:41<00:00, 10.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ave_Train_loss 0.5031819641590118\n",
            "Train F1 (micro) 0.06530127634312852\n",
            "Train Hamming loss 0.11662962962962962\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss 0.46235212683677673\n",
            "Validation F1 (micro) 0.0562968378724182\n",
            "Validation Hamming loss 0.0956111111111111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch#5: 100%|██████████| 4/4 [00:41<00:00, 10.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ave_Train_loss 0.46050839871168137\n",
            "Train F1 (micro) 0.05758454106280193\n",
            "Train Hamming loss 0.09031481481481482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss 0.4211605563759804\n",
            "Validation F1 (micro) 0.036806883365200764\n",
            "Validation Hamming loss 0.07462962962962963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch#6: 100%|██████████| 4/4 [00:42<00:00, 10.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ave_Train_loss 0.4225640222430229\n",
            "Train F1 (micro) 0.02666666666666667\n",
            "Train Hamming loss 0.06894444444444445\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss 0.38549066334962845\n",
            "Validation F1 (micro) 0.013268998793727381\n",
            "Validation Hamming loss 0.060592592592592594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch#7: 100%|██████████| 4/4 [00:42<00:00, 10.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ave_Train_loss 0.3889523297548294\n",
            "Train F1 (micro) 0.007389989922741014\n",
            "Train Hamming loss 0.05472222222222222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss 0.35490595549345016\n",
            "Validation F1 (micro) 0.00939306358381503\n",
            "Validation Hamming loss 0.050777777777777776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch#8: 100%|██████████| 4/4 [00:41<00:00, 10.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ave_Train_loss 0.3597874864935875\n",
            "Train F1 (micro) 0.007527286413248024\n",
            "Train Hamming loss 0.04883333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss 0.3288890868425369\n",
            "Validation F1 (micro) 0.006235385814497273\n",
            "Validation Hamming loss 0.04722222222222222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch#9: 100%|██████████| 4/4 [00:41<00:00, 10.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ave_Train_loss 0.33521369099617004\n",
            "Train F1 (micro) 0.0031733439111463714\n",
            "Train Hamming loss 0.046537037037037036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss 0.3063978999853134\n",
            "Validation F1 (micro) 0.006274509803921569\n",
            "Validation Hamming loss 0.046925925925925926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train epoch#10: 100%|██████████| 4/4 [00:41<00:00, 10.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ave_Train_loss 0.31433022022247314\n",
            "Train F1 (micro) 0.005443234836702954\n",
            "Train Hamming loss 0.04737037037037037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                           "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss 0.28740374743938446\n",
            "Validation F1 (micro) 0.005505308690523005\n",
            "Validation Hamming loss 0.04683333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    }
  ]
}